---
layout: single
title: "🔬 LLM 압축 기술 (Quantization, Pruning)"
date: 2026-02-19 10:00:00 +0900
categories:
  - 연구논문
tags:
  - "AI연구"
  - "논문"
author_profile: false
read_time: true
---

## LLM 압축의 필요성

GPT-4 수준의 대형 언어모델은 수천억 개의 파라미터와 수백 GB의 메모리를 요구해 일반 하드웨어에서 구동이 불가능하다. LLM 압축 기술은 모델 크기와 연산량을 줄이면서 성능 손실을 최소화해 온디바이스·엣지 배포를 가능하게 한다.

## 양자화(Quantization)

양자화는 부동소수점(FP16/FP32) 가중치를 더 낮은 비트 정수(INT8, INT4, 심지어 1~2비트)로 변환해 메모리와 연산을 줄이는 기법이다. GPTQ, AWQ, GGUF 형식의 양자화가 널리 쓰이며, Ollama·llama.cpp 등의 도구로 양자화 모델을 일반 PC에서 실행할 수 있다. 4비트 양자화 시 메모리를 75% 줄이면서 성능 손실은 5% 이하로 유지하는 수준에 도달했다.

## 가지치기(Pruning)와 지식 증류(Knowledge Distillation)

Pruning은 중요도가 낮은 가중치·뉴런·레이어를 제거해 모델을 축소한다. Structured Pruning(레이어 전체 제거)과 Unstructured Pruning(개별 가중치 제거)이 있다. Knowledge Distillation은 대형 교사(Teacher) 모델의 지식을 소형 학생(Student) 모델에 전수한다. Microsoft Phi-3가 Knowledge Distillation의 성공 사례로 꼽힌다.

## 2026년 압축 연구 전망

1~2비트 초저비트 양자화, 레이어 선택적 양자화, 동적 압축(입력에 따라 압축률 조절) 등이 활발히 연구된다. 압축 기술의 발전은 온디바이스 AI 확산을 가속하고, 추론 비용을 획기적으로 낮춰 AI 서비스 대중화를 이끌 전망이다.
