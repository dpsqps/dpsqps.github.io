---
layout: single
title: "🔬 RLHF vs DPO AI 정렬 기법 비교"
date: 2026-02-12 10:00:00 +0900
categories:
  - 연구논문
tags:
  - "AI연구"
  - "논문"
author_profile: false
read_time: true
---

## AI 정렬(Alignment)이란

AI 정렬은 LLM이 인간의 가치·의도·선호와 일치하는 방식으로 행동하도록 훈련하는 연구 분야다. 단순히 문장을 완성하는 수준을 넘어 도움이 되고(Helpful), 무해하며(Harmless), 정직한(Honest) AI를 만드는 것이 핵심 목표다.

## RLHF(인간 피드백 강화학습)

RLHF는 InstructGPT(2022)에서 상용화된 기법이다. ① 지도 학습으로 기본 모델 파인튜닝 → ② 사람이 모델 출력을 비교 평가해 보상 모델 훈련 → ③ PPO 알고리즘으로 보상을 최대화하도록 정책 최적화의 3단계로 구성된다. ChatGPT·Claude·Gemini 모두 RLHF 계열 기법을 사용한다. 단점은 보상 모델 훈련·PPO 알고리즘의 복잡성과 불안정성이다.

## DPO(직접 선호 최적화)

2023년 Stanford가 제안한 DPO는 별도의 보상 모델 없이 선호 데이터(선호 응답·비선호 응답 쌍)로 직접 LLM을 최적화한다. RLHF 대비 훈련이 단순·안정적이고 메모리 효율이 좋아 오픈소스 모델 파인튜닝에 널리 쓰인다. 그러나 RLHF만큼의 정렬 품질에는 아직 미치지 못한다는 연구도 있다.

## 최신 연구 동향

RLAIF(AI 피드백 강화학습)는 인간 평가자 대신 AI(Claude 등)가 출력을 평가해 보상 신호를 생성한다. SimPO, ORPO 등 DPO 개선 기법도 활발히 연구 중이다. Anthropic의 Constitutional AI는 규칙 기반 자기 비판을 통해 정렬을 달성하는 독창적 접근이다.

*👁‍🗨 AI 트렌드 레이더 자동 생성*
