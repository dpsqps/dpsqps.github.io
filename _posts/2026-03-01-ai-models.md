---
layout: single
title: "🤖 AI 모델 업데이트 — 2026년 03월 01일"
date: 2026-03-01 09:01:00 +0900
categories:
  - AI모델
tags:
  - "AI모델"
  - "LLM"
  - "OpenAI"
  - "ChatGPT"
  - "Gemini"
  - "MoE"
  - "GPT"
author_profile: false
read_time: true
header:
  overlay_image: "https://picsum.photos/seed/ai-language-model/1600/500"
  overlay_filter: "0.6"
---

## OpenAI 195조 투자 유치 & ChatGPT 9억 사용자 돌파

2026년 2월 마지막 주, AI 모델 업계에 역사적인 기록들이 쏟아졌습니다.

### OpenAI, 사상 최대 1,100억 달러 투자 유치

OpenAI가 소프트뱅크(300억 달러), 엔비디아(300억 달러), 아마존(500억 달러) 등으로부터 총 1,100억 달러(약 195조 원) 규모의 투자를 유치했습니다. 기업 가치는 7,300억 달러(약 1,053조 원)로 평가돼 역사상 최대 민간 투자 신기록을 세웠습니다. 동시에 아마존과는 8년간 1,000억 달러에 달하는 별도 파트너십도 체결했습니다. [TechCrunch](https://techcrunch.com/2026/02/27/openai-raises-110b-in-one-of-the-largest-private-funding-rounds-in-history/)

### ChatGPT 주간 활성 사용자 9억 명 돌파

ChatGPT의 주간 활성 사용자(WAU)가 9억 명을 돌파했습니다. 불과 수개월 만에 급격한 성장을 기록하며, AI 챗봇이 인터넷 검색에 버금가는 일상 필수 도구로 자리잡고 있음을 보여줍니다. [TechCrunch](https://techcrunch.com/2026/02/27/chatgpt-reaches-900m-weekly-active-users/)

### 구글, 온디바이스 이미지 생성 모델 나노-바나나 2 공개

구글이 스마트폰에서 0.5초 이내 고해상도 이미지를 생성하는 온디바이스 AI 모델 나노-바나나 2(Gemini 3.1 Flash Image)를 공개했습니다. 18억 개의 매개변수로 구성된 이 모델은 클라우드 없이 기기 내부에서 모든 연산을 처리합니다. [AI타임스](https://www.aitimes.com/news/articleView.html?idxno=207354)

### Hugging Face: MoE 아키텍처 심층 분석

Hugging Face 블로그에서는 트랜스포머에서의 Mixture of Experts(MoE) 아키텍처를 심층 분석한 기술 글이 게재됐습니다. MoE는 현재 가장 강력한 LLM들이 채택하는 핵심 기술로, 효율적 스케일링의 열쇠로 주목받고 있습니다. [Hugging Face](https://huggingface.co/blog/moe-transformers)

이번 주 AI 모델 분야는 기록적인 투자와 사용자 성장, 온디바이스 AI의 발전이라는 세 축으로 정리됩니다. OpenAI의 압도적 자금력은 향후 AI 경쟁 구도를 더욱 심화시킬 전망입니다.

---

## 📎 참고 자료

1. [OpenAI raises $110B in one of the largest private funding rounds in history](https://techcrunch.com/2026/02/27/openai-raises-110b-in-one-of-the-largest-private-funding-rounds-in-history/)
2. [ChatGPT reaches 900M weekly active users](https://techcrunch.com/2026/02/27/chatgpt-reaches-900m-weekly-active-users/)
3. [나노 바나나 2 모바일에서 500밀리초 미만 지연 시간](https://www.aitimes.com/news/articleView.html?idxno=207354)
4. [Mixture of Experts (MoEs) in Transformers](https://huggingface.co/blog/moe-transformers)

