---
layout: single
title: "🔬 멀티모달 Vision-Language Model"
date: 2026-02-16 10:00:00 +0900
categories:
  - 연구논문
tags:
  - "AI연구"
  - "논문"
author_profile: false
read_time: true
---

## Vision-Language Model 연구 현황

Vision-Language Model(VLM)은 이미지와 텍스트를 동시에 이해하고 생성하는 멀티모달 AI다. CLIP(OpenAI, 2021)이 이미지-텍스트 연결 방법을 혁신한 이후, GPT-4V, LLaVA, InternVL, Qwen-VL, Google Gemini 등 강력한 VLM이 연달아 등장했다.

## 주요 연구 성과

CLIP은 4억 쌍의 이미지-텍스트 데이터로 이미지와 텍스트를 같은 임베딩 공간에 표현하는 방법을 학습했다. DALL-E·Stable Diffusion의 기반이 됐다. LLaVA(Large Language and Vision Assistant)는 CLIP 비전 인코더와 LLaMA를 연결해 시각 질문 응답(VQA)·이미지 설명 생성 능력을 오픈소스로 공개했다. InternVL 2.5는 OCR, 차트 이해, 수학 문제 풀이에서 GPT-4V를 앞선다는 평가를 받았다.

## 주요 연구 과제

**세밀한 이해(Fine-grained Understanding):** 복잡한 장면의 공간 관계, 작은 텍스트 인식에서 아직 인간 수준에 미치지 못한다. **환각:** 이미지에 없는 내용을 텍스트로 생성하는 문제. **일관된 생성:** 텍스트 설명을 이미지로 정확히 구현하는 것, 특히 복잡한 속성 결합. **비디오 이해:** 긴 동영상의 시간적 추론과 인과 관계 파악.

## 2026년 전망

비디오 이해 VLM, 3D 장면 이해, 의료 이미징 특화 VLM이 활발히 연구되고 있다. 온디바이스 경량 VLM도 빠르게 발전해 스마트폰에서 실시간 시각 AI 어시스턴트가 일반화될 전망이다.

---

## 📎 참고 자료

1. [LLaVA: Large Language and Vision Assistant | arXiv](https://arxiv.org/abs/2304.10592)
2. [BLIP-2: Bootstrapping Language-Image Pre-training | arXiv](https://arxiv.org/abs/2301.12597)
3. [A Comprehensive Review of Multimodal LLMs | arXiv](https://arxiv.org/abs/2408.01319)
