---
layout: single
title: "🔬 Anthropic Constitutional AI 안전성 연구"
date: 2026-02-15 10:00:00 +0900
categories:
  - 연구논문
tags:
  - "AI연구"
  - "논문"
author_profile: false
read_time: true
---

## Constitutional AI(CAI) 개요

Constitutional AI(CAI)는 2022년 Anthropic이 발표한 AI 정렬 기법이다. 기존 RLHF가 인간 피드백에 의존하는 것과 달리, AI 스스로 미리 정의된 '헌법(원칙 목록)'에 따라 자신의 출력을 비판·수정한다. 인간 감독 노력을 줄이면서도 더 일관된 안전성을 달성하는 것이 목표다.

## CAI의 동작 방식

1단계(지도 학습): AI가 유해한 응답을 생성한 뒤, 헌법 원칙("해롭지 않아야 한다", "정직해야 한다" 등)에 따라 자기 비판을 수행하고 수정 응답을 생성한다. 이 쌍으로 파인튜닝한다. 2단계(RL from AI Feedback, RLAIF): AI가 응답 쌍의 선호도를 헌법에 근거해 판단하는 보상 모델을 훈련하고, 이를 RL에 활용한다. Claude 모델 시리즈가 CAI를 기반으로 훈련됐다.

## 연구 성의와 한계

CAI는 인간 레이블러의 편향을 줄이고, 확장 가능한 감독(Scalable Oversight) 방법으로 주목받는다. 그러나 헌법 자체의 설계가 AI 행동에 큰 영향을 미치며, 누가 헌법을 정의하느냐에 따른 가치 주입 문제도 존재한다. Anthropic은 사용자·운영자·회사 3자의 신뢰 계층을 도입한 개선된 정렬 연구를 지속하고 있다.

## AI 안전성 연구의 미래

Anthropic의 Responsible Scaling Policy(RSP)는 AI 능력 수준별 안전 조건을 명시한 선구적 사례다. 2026년 AI 안전성 연구는 해석 가능성(Interpretability), 사실성(Factuality), 자율 AI 에이전트의 위험 관리로 확장될 전망이다.

---

## 📎 참고 자료

1. [Constitutional AI: Harmlessness from AI Feedback | arXiv](https://arxiv.org/abs/2212.08073)
2. [Claude's Constitution | Anthropic](https://www.anthropic.com/news/claudes-constitution)
