---
layout: single
title: "🔬 Mixture of Experts (MoE) 아키텍처"
date: 2026-02-13 10:00:00 +0900
categories:
  - 연구논문
tags:
  - "AI연구"
  - "논문"
author_profile: false
read_time: true
---

## MoE 아키텍처 개요

Mixture of Experts(MoE)는 모델을 여러 전문가(Expert) 서브네트워크로 분할하고, 입력마다 소수의 전문가만 활성화하는 조건부 연산 구조다. 전체 파라미터는 많지만 연산에 사용하는 파라미터는 적어 계산 효율이 높다. GPT-4, Gemini 1.5, Mixtral 8x7B, DeepSeek V2·V3 등 최신 대형 모델에 광범위하게 적용됐다.

## MoE의 동작 원리

게이팅 네트워크(Router)가 각 토큰에 대해 적합한 전문가 K개를 선택한다. 예를 들어 Mixtral 8x7B는 8개의 전문가 중 2개만 활성화해 실질적으로 12.9B 파라미터를 사용하지만 47B 모델의 성능을 낸다. 전문가들은 수학·코딩·언어 등 특정 패턴에 자연스럽게 특화되는 경향이 있다.

## 주요 연구 성과

Google Switch Transformer(2021)는 MoE를 트랜스포머에 효율적으로 통합하는 방법을 제시했다. DeepSeek V3는 MoE와 멀티헤드 잠재 어텐션(MLA)을 결합해 훈련 비용을 대폭 절감하며 GPT-4급 성능을 달성했다. 전문가 부하 불균형(Load Imbalance) 문제를 해결하는 보조 손실 함수 연구도 활발하다.

## 한계와 미래

MoE는 분산 학습 시 전문가 간 통신 비용이 크고, 서빙 시 전체 모델을 메모리에 올려야 해 추론 인프라 요구가 높다. 이를 해결하는 Expert Offloading, Sparse MoE 서빙 최적화 연구가 이어지고 있다. 2026년에는 MoE가 사실상 대형 모델의 표준 아키텍처로 자리 잡을 전망이다.
